{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561e9739-4784-4543-83b9-24200af2a07a",
   "metadata": {},
   "source": [
    "# Modern Experimental Physics 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b24b7-ef89-4559-9f52-d0af7ce1159c",
   "metadata": {},
   "source": [
    "## Exercise sheet 4: Direct Dark Matter Detection \n",
    "This exercise teaches how to analyse the data taken at a Direct Detection Dark Matter Experiment such as the superfluid helium-based \"Direct search Experiment for Light DM\", also called DELight. At the beginning, a short introduction to magnetic microcalorimeters (MMCs) is provided. The theoretical expectations for the shapes of the K$_\\alpha$ and K$_\\beta$ lines of the $^{55}$Fe source used to test the MMC are the subject of Exercise 1. Exercise 2 introduces the traces recorded with the MMC. In Exercise 3, a simplified event selection is performed to select clean events and to improve the resolution of the measurement. In Exercise 4, a time correction is performed to compensate for the rising temperature of the MMC during the measurement. In Exercise 5, a $\\chi^2$ fit is used to perform the energy calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995c94c-8fdb-4272-88f7-752511261535",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "A magnetic microcalorimeter (MMC) is a temperature sensor that exploits magnetization effects to detect the miniscule temperature increases caused by the absorption of X-rays or other highly energetic particles. The proposed DELight experiment will employ an MMC to detect collisions of light Dark Matter with Standard Model particles. In preperation of this experiment, this exercise sheet based on [this paper](https://arxiv.org/pdf/2310.08512) uses a $^{55}$Fe X-ray source to calibrate the detector and employs an optimum filter-based amplitude estimation to characterize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c90fb-2c90-460f-a060-9a3d07f12edb",
   "metadata": {},
   "source": [
    "<img src=\"MMC_calibration.jpg\" style=\"margin-left:auto; margin-right:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baefb1e-d386-4c09-a820-9218f0d909fb",
   "metadata": {},
   "source": [
    "### Exercise 1: Theoretical Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b5107-f1ae-4bb9-b9eb-b1a5a07a6bf0",
   "metadata": {},
   "source": [
    "The $^{55}$Fe source used to calibrate the magnetic microcalorimeter undergoes electron capture, which transforms it into $^{55}$Mn. The resulting vacancy usually lies in the K-shell and is filled by an electron from a higher shell, causing a cascade of X-rays and auger electrons. A beryllium window in front of the source blocks the less energetic radiation from other de-excitations into other shells. The X-rays that reach the MMC are emitted by electrons from either the L- or M-shell filling the vacancy in the K-shell, called K$_\\alpha$ and K$_\\beta$ transitions respectively. In the following exercises, the K$_\\alpha$ and K$_\\beta$ spectral lines are used to calibrate the MMC and characterize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c0427-3dcb-40fb-bf4b-aba816e786e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Question:</strong></br>\n",
    "The spectral lines are modeled using Voigt distributions, multiplied with error functions to account for athermal phonon escape (phonons escaping into the substrate of the MMC before they can thermalize). How is the Voigt distribution defined and why is it typically used in spectroscopy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100321a-fb04-4d34-8584-549c844b3122",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><strong>Answer:</strong></br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3a774-c24f-41bc-aeed-103e1fc01ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iminuit\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import cauchy as lorentz\n",
    "from scipy.special import voigt_profile as voigt\n",
    "from scipy.special import erf, erfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a235d-1d82-4379-9da7-a0f17a9c9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the theoretical model\n",
    "normalize = lambda a: a / np.sum(a)\n",
    "\n",
    "a_ij = [5898.853, 5897.867, 5894.829, 5896.532, 5899.417, 5902.712, 5887.743, 5886.495]\n",
    "aw_ij = [1.715, 2.043, 4.499, 2.663, 0.969, 1.5528, 2.361, 4.216] # FWHM\n",
    "ai_ij = [0.790, 0.264, 0.068, 0.096, 0.007, 0.0106, 0.372, 0.100]\n",
    "b_ij = [6490.89, 6486.31, 6477.73, 6490.06, 6488.83]\n",
    "bw_ij = [1.83, 9.40, 13.22, 1.81, 2.81]\n",
    "bi_ij = [0.608, 0.109, 0.077, 0.397, 0.176]\n",
    "\n",
    "skewed_lorentz = np.vectorize(lambda x, s, l: quad(lambda x, theta=x, s=s, l=l: skewed_lorentz_ft(x, theta, s, l), 0, np.inf)[0])\n",
    "\n",
    "K_alpha_pdf_esc = np.vectorize(lambda x, sigma, esc_k: np.sum([ai_ij[i]*voigt(x - a_ij[i], sigma, aw_ij[i]/2)*erfc(esc_k*(x - a_ij[i]))/voigt(0, sigma, aw_ij[i]/2) for i in range(len(a_ij))])/np.sum([ai_ij[i]/voigt(0, sigma, aw_ij[i]/2) for i in range(len(a_ij))]))\n",
    "K_beta_pdf_esc = np.vectorize(lambda x, sigma, esc_k: np.sum([bi_ij[i]*voigt(x - b_ij[i], sigma, bw_ij[i]/2)*erfc(esc_k*(x - b_ij[i]))/voigt(0, sigma, bw_ij[i]/2) for i in range(len(b_ij))])/np.sum([bi_ij[i]/voigt(0, sigma, bw_ij[i]/2) for i in range(len(b_ij))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53d00f-d339-4031-bf94-7321eda31840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the theoretical model for the K_alpha line\n",
    "Es = np.linspace(5870, 5920, 2000)\n",
    "esc_k = 0.\n",
    "plt.plot(Es, K_alpha_pdf_esc(Es, 0.7, esc_k), lw=2., color='xkcd:black')\n",
    "plt.xlim(5870, 5920), plt.ylim(0, 0.15)\n",
    "plt.xlabel('Energy [eV]'), plt.ylabel('arbitrary units')\n",
    "plt.title(r'theoretical K$_\\alpha$ spectrum')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d331fb-a4f6-4fd8-9aaa-dad08da30681",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Question:</strong></br>\n",
    "Why is the K$_\\alpha$ line split into two peaks?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7428aaf-6b03-4629-aff3-e96c3343f08f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><strong>Answer:</strong></br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799191f-1da5-48e6-b981-d150fd4ebeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the theoretical model for the K_beta line\n",
    "Es = np.linspace(6450, 6520, 2000)\n",
    "esc_k = 0.\n",
    "plt.plot(Es, K_beta_pdf_esc(Es, 0.7, esc_k), lw=2., color='xkcd:black')\n",
    "plt.xlim(6450, 6520), plt.ylim(0, 0.15)\n",
    "plt.xlabel('Energy [eV]'), plt.ylabel('arbitrary units')\n",
    "plt.title(r'theoretical K$_\\beta$ spectrum')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca74c7-ef1e-4360-9822-04e9cfdafdc0",
   "metadata": {},
   "source": [
    "### Exercise 2: Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850ebb6-1d4e-4c44-89af-98464c3c861f",
   "metadata": {},
   "source": [
    "The X-rays emitted during the K$_\\alpha$ and K$_\\beta$ transitions in the $^{55}$Fe calibration source hit the absorber, resulting in a temperature increase $\\delta T = \\delta E/C_\\mathrm{tot}$ in the thermally coupled paramagnetic temperature sensor. The detector is operated at mK temperatures, reducing the heat capacity $C_\\mathrm{tot}$ of the system to a point, where even a small energy deposition leads to a large increase in temperature. The magnetization of the paramagnetic sensor strongly depends on its temperature, which means that an energy deposition leads to a variation of the magnetic flux. A direct current superconducting quantum interference device (dc-SQUID) is used to convert the change in the magnetic flux into a voltage change, which in turn is converted into a digital signal by an analog-to-digital converter (ADC). The resulting ADC counts are stored in the form of so-called traces. These traces have a total length of 32768 data points, taken over a duration of about 8.4 ms, with the triggering time at around 1 ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f326c-6da2-4b1b-8e0f-9dc35cc3478d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Question:</strong></br>\n",
    "What do you expect the traces to typically look like?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6839ef-d13b-4545-b379-daafbf31aa7c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><strong>Answer:</strong></br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479b0c4-53c3-45b8-8712-8214a88d0244",
   "metadata": {},
   "source": [
    "**Task:** A subset of 100 traces is saved in `traces.csv.gz`, with each row containing the ADC count values for one trace. Plot the traces while calibrating the x-axis to show the correct time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484ff4c-6586-42dc-b100-c074970ec63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pd.read_csv('traces.csv.gz', header=None)\n",
    "\n",
    "# Your code ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad94293-0279-4851-9027-7cdf0e546719",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><strong>Question:</strong></br>\n",
    "Which traces are caused by background processes and should not be considered for further analysis? What could cause these events?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f7631-d69c-40be-93db-68cd5fb55a62",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><strong>Answer:</strong></br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e26469-7514-4dc8-976f-61a28c265a63",
   "metadata": {},
   "source": [
    "### Exercise 3: Event Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e1f08-aa02-41e7-8e53-293b2fd93c0d",
   "metadata": {},
   "source": [
    "To improve the resolution, an optimum filter (OF) is applied before the energy reconstruction. This filter minimizes a $\\chi^2$ value in the frequency domain, defined as \n",
    "$$\\chi^2 = \\sum_\\nu \\frac{[S(\\nu)-aA(\\nu)]^2}{J(\\nu)},$$\n",
    "where $S(\\nu)$ is the signal, $A(\\nu)$ is the signal template scaled with an amplitude $a$ and $J(\\nu)$ is the power spectrum density of the noise. The necessary signal templates $A(\\nu)$  are obtained by averaging a set of traces and fitting a template, while the noise $J(\\nu)$ is obtained by subtracting the templates from the traces. To improve the performance, two templates based on K$_\\alpha$ and K$_\\beta$ traces are used to perform the OF fit, resulting in the two filters OF$_\\alpha$ and OF$_\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28babe-733a-4bd7-a1b7-b31f0cf54cea",
   "metadata": {},
   "source": [
    "An event selection is applied to remove the bad events discussed in the previous exercise. While most of the selection cuts to quantities such as the mean and standard deviation of the baseline, the temperature and the OF amplitudes are already applied, your task is to find an appropriate cut on the baseline offset. The baseline offset is an indicator for the temperature of the detector in the moments prior to the absorption of X-rays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a0325-0546-4656-976b-80553fae8bf1",
   "metadata": {},
   "source": [
    "**Task:** The file `rqs.csv.gz` contains different variables describing properties of the traces. \n",
    "- Plot a histogram of the `baseline_offset`.\n",
    "- Find appropriate cuts to ensure a stable baseline offset without removing too many events.\n",
    "- Plot the `OF_ampl_0` and `OF_ampl_1` distributions around the $K_\\alpha$ and $K_\\beta$ lines respectively, with and without this cut.\n",
    "- Calculate the efficiency of the cut to determine how many events are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4d2cb-50e8-44f7-b651-2d1f5a53b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('rqs.csv.gz')\n",
    "\n",
    "# Your code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2c570-0b26-4dd9-a082-1b5661fc1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a cut on the baseline offset\n",
    "mask_baseline_offset = (df['baseline_offset'] > ...) & (df['baseline_offset'] < ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67981387-1688-41e3-b7c2-d57fc6f464d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_alpha = np.linspace(1.035e4, 1.045e4, 101)\n",
    "binning_beta = np.linspace(1.132e4, 1.142e4, 76)\n",
    "\n",
    "# Your code ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d5191-e866-4ced-9375-85912711f2d1",
   "metadata": {},
   "source": [
    "### Exercise 4: Time Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7ed7b-8d69-493a-a416-851e7428810e",
   "metadata": {},
   "source": [
    "The following steps of the analysis should be performed both with and without the baseline offset cuts chosen in the previous exercise to inspect the impact of the cuts on the final result for the resolution. If the final fits do not converge, you should try different cut values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ab2ff-4ac3-4e4d-bde4-693aca6d6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a selection mask here to easily enable and disable the baseline offset cut by commenting one of the lines and rerunning the notebook\n",
    "mask_sel = np.ones_like(df['time'],dtype=bool)\n",
    "#mask_sel = mask_baseline_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b43c72-2bf9-4d7d-8b00-37cf6e3d4c66",
   "metadata": {},
   "source": [
    "Over the course of the calibration, which took around 50 minutes, the OF amplitudes steadily decreased, most likely due to a temperature drift of either the detector itself or the electronics. This effect is well described by a first-order polynomial. To account for this, a time-dependent correction is applied by fitting both OF$_\\alpha$ and OF$_\\beta$ to the K$_\\alpha$ line and correcting for the time-dependence.\n",
    "\n",
    "**Task:** \n",
    "- Plot the OF amplitude at the K$_\\alpha$ line over the time for OF$_\\alpha$ and OF$_\\beta$ in the form of a two-dimensional histogram to observe the slight shift of the peaks. \n",
    "- Calculate the mean amplitudes for each time bin and parametrize the time dependence through a linear fit.\n",
    "- Use this fit to calculate a corrected OF amplitude for both OF$_\\alpha$ and OF$_\\beta$. This is done by calculating the ratio of the original amplitude $a$ and the amplitude resulting from the Fit $a_\\mathrm{fit}(t)$ and multiplying it with the mean $\\mu$ of all the amplitudes used to perform the linear fit:\n",
    "$$a_\\mathrm{cOF} = \\frac{a}{a_\\mathrm{fit}(t)} \\cdot \\mu,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e4c86-9562-4c49-9095-c03de8d12f4a",
   "metadata": {},
   "source": [
    "**Hint:** The function `stats.binned_statistic` can be useful to calculate statistics for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9002e-22c9-435d-9dcd-41d9576c022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D histogram and linear fit for OF_alpha\n",
    "# this mask selects the area around the K_alpha line\n",
    "mask_peak = (df.OF_ampl_0 > 1.036e4)&(df.OF_ampl_0 < 1.044e4)\n",
    "\n",
    "# Your code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607afcf7-55ed-49f2-a36f-5e7b39d09beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the corrected amplitude using OF_alpha\n",
    "\n",
    "df['cOF_ampl_0'] = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc08e96-ef99-45f0-8d13-3aec44fbbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D histogram and linear fit for OF_beta\n",
    "# this mask selects the area around the K_alpha line\n",
    "mask_peak = (df.OF_ampl_1 > 1.036e4)&(df.OF_ampl_1 < 1.044e4)\n",
    "\n",
    "# Your code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff7dc0-cce6-4292-bb37-6d689232a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the corrected amplitude using OF_beta\n",
    "\n",
    "df['cOF_ampl_1'] = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4717d-10d3-4465-a897-79b5475f0288",
   "metadata": {},
   "source": [
    "The two corrected amplitudes are then combined into a general corrected OF amplitude that uses the corrected OF$_\\alpha$  amplitude around the K$_\\alpha$ peak and the corrected OF$_\\beta$ amplitude around the K$_\\beta$ peak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa23581-97db-423f-8af0-d4b4571caddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only areas around the K_alpha and K_beta lines for the upcoming fits\n",
    "mask_fit = mask_sel & (((df.cOF_ampl_0 > binning_alpha[0])&((df.cOF_ampl_0 < binning_alpha[-1])))|((df.cOF_ampl_0 > binning_beta[0])&((df.cOF_ampl_0 < binning_beta[-1]))))\n",
    "df = df[mask_fit]\n",
    "\n",
    "# create a combined cOF, using OF_alpha around K_alpha and OF_beta around K_beta\n",
    "df['cOF_ampl'] = np.zeros_like(df.cOF_ampl_0)\n",
    "df.loc[((df.cOF_ampl_0 > binning_alpha[0])&((df.cOF_ampl_0 < binning_alpha[-1]))),'cOF_ampl'] = df.cOF_ampl_0[((df.cOF_ampl_0 > binning_alpha[0])&((df.cOF_ampl_0 < binning_alpha[-1])))]\n",
    "df.loc[((df.cOF_ampl_0 > binning_beta[0])&((df.cOF_ampl_0 < binning_beta[-1]))), 'cOF_ampl'] = df.cOF_ampl_1[((df.cOF_ampl_0 > binning_beta[0])&((df.cOF_ampl_0 < binning_beta[-1])))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d5c72-88d0-4521-b396-9ce3f56b18d8",
   "metadata": {},
   "source": [
    "**Task:** Plot histograms of the corrected OF amplitude around the K$_\\alpha$ and K$_\\beta$ peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c208e-88f2-4f9a-8332-ea48f88ca149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of the cOF at K_alpha and K_beta\n",
    "binning_alpha = np.linspace(1.035e4, 1.045e4, 101)\n",
    "binning_beta = np.linspace(1.132e4, 1.142e4, 76)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ed44b-1c7d-4b18-a7a6-b9f9a44a2f6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 5: Energy Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40189e5-6074-4b3b-b86f-28f6839ba722",
   "metadata": {},
   "source": [
    "Finally, a binned $\\chi^2$ fit is performed to calibrate the OF amplitudes to energies. The spectral shape of the lines is modeled as a superposition of Voigt distributions and error functions, as discussed earlier. The calibration function correlating the corrected OF amplitude $A_\\mathrm{cOF}$ to the energy is given as \n",
    "$$ E = p_1 \\cdot A^2_\\mathrm{cOF} + p_2 \\cdot A_\\mathrm{cOF}. $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffd2dc-20bc-4d45-82db-8c451b8c9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the chi2 value\n",
    "def chi2(hist_obs, hist_model, sigma):\n",
    "    return np.sum((hist_obs - hist_model)**2/sigma**2)\n",
    "\n",
    "#the fit function used to model the K_alpha line\n",
    "def model_alpha(x, mu):\n",
    "    A, E_2, E_1, E_0, sigma_0, esc_k = mu\n",
    "    E = E_2 * x**2 + E_1 * x + E_0\n",
    "    return ka * K_alpha_pdf_esc(E, sigma_0, esc_k)\n",
    "\n",
    "#construct the histogram from the observed data\n",
    "def get_hist_obs(Es, binning):\n",
    "    hist_obs, bin_edges = np.histogram(Es, bins=binning)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    return bin_centers, hist_obs\n",
    "\n",
    "#return the fit function with the correct binning for the K_alpha line\n",
    "def get_hist_model(binning, ka, sigma_0, esc_k, c):\n",
    "    bin_centers = 0.5 * (binning[:-1] + binning[1:])\n",
    "    return bin_centers, ka * K_alpha_pdf_esc(bin_centers, sigma_0, esc_k) + c\n",
    "\n",
    "#return the fit function with the correct binning for the K_beta line\n",
    "def get_hist_model_beta(binning, ka, sigma_0, esc_k, c):\n",
    "    bin_centers = 0.5 * (binning[:-1] + binning[1:])\n",
    "    return bin_centers, K_beta_pdf_esc(bin_centers, sigma_0, esc_k) + c\n",
    "\n",
    "#perform the binned chi2 fit at the K_alpha line\n",
    "def fit_model_alpha(obs, initial_values, binning, empty=False):\n",
    "    \n",
    "    def _minimize_function(x, ka, E_2, E_1, E_0, sigma_0, esc_k, c, binning=binning):\n",
    "        Es = E_2 * x**2 + E_1 * x + E_0\n",
    "        binning = np.linspace(*binning)\n",
    "        E_binning = E_2 * binning**2 + E_1 * binning + E_0\n",
    "        bin_centers, hist_obs = get_hist_obs(Es, E_binning)\n",
    "        _, hist_model = get_hist_model(E_binning, ka, sigma_0, esc_k, c)\n",
    "        return chi2(hist_obs, hist_model, np.sqrt(hist_obs))\n",
    "    \n",
    "    func = lambda ka, E_2, E_1, E_0, sigma_0, esc_k, c: _minimize_function(obs, ka, E_2, E_1, E_0, sigma_0, esc_k, c, binning)\n",
    "    m = iminuit.Minuit(func, **initial_values)\n",
    "    if not empty:\n",
    "        m.fixed[\"ka\"] = False\n",
    "        m.fixed[\"sigma_0\"] = False\n",
    "        m.fixed[\"E_2\"] = False\n",
    "        m.fixed[\"E_1\"] = False\n",
    "        m.fixed[\"E_0\"] = True\n",
    "        m.fixed[\"esc_k\"] = False\n",
    "        m.fixed[\"c\"] = True\n",
    "\n",
    "        m.simplex().migrad()\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5969f-2184-4d38-ad58-bd2b730eb5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for K_alpha, plot the fit\n",
    "#for K_beta, perform an amplitude fit while using values from the K_alpha fit for the other parameters, then plot the fit\n",
    "def plot_spectrum_fit(df, model, Kline='alpha', ADC_bins=None, E_bins=None):\n",
    "    \n",
    "    ADC_to_E = lambda x: model.values['E_2'] * x**2 + model.values['E_1'] * x + model.values['E_0']\n",
    "    if ADC_bins == None:\n",
    "        binning = np.linspace(*E_bins)\n",
    "    else:\n",
    "        binning = ADC_to_E(np.linspace(*ADC_bins))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    plt.subplots_adjust(hspace=0.05)\n",
    "\n",
    "    plt.sca(ax1)\n",
    "    hist, bin_edges = np.histogram(ADC_to_E(df.cOF_ampl), bins=binning)\n",
    "    bin_centers = 0.5 * (binning[:-1] + binning[1:])\n",
    "    plt.hist(ADC_to_E(df.cOF_ampl), bins=binning, histtype='step', label='data')\n",
    "    plt.errorbar(bin_centers, hist, yerr=np.sqrt(hist), ls='None', c='C0')\n",
    "\n",
    "    get_model = get_hist_model_beta if Kline == 'beta' else get_hist_model\n",
    "    bin_centers_model, y_model = get_model(binning, model.values['ka'], model.values['sigma_0'], model.values['esc_k'], model.values['c'])\n",
    "    if Kline == 'beta':\n",
    "        m_beta = iminuit.Minuit(lambda a: chi2(hist, a * y_model, np.sqrt(hist)), a = 1e2)\n",
    "        m_beta.simplex().migrad()\n",
    "        y_model = m_beta.values['a'] * y_model\n",
    "    Es = np.linspace(binning[0], binning[-1], 1001)\n",
    "    bin_centers_model, y_model_plot = get_model(Es, model.values['ka'], model.values['sigma_0'], model.values['esc_k'], model.values['c'])\n",
    "    y_model_plot = m_beta.values['a'] * y_model_plot if Kline == 'beta' else y_model_plot\n",
    "    plt.plot(bin_centers_model, y_model_plot, label='fit')\n",
    "    plt.xlim(binning[0], binning[-1]);\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.sca(ax2)\n",
    "    residuals = hist - y_model\n",
    "    plt.axhline(0, color='xkcd:black', lw=0.5, ls='--')\n",
    "    plt.scatter(bin_centers, residuals / np.sqrt(hist), s=2.)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.ylabel(r'Res / $\\sigma$'), plt.xlabel('Energy [eV]')\n",
    "    if Kline == 'beta':\n",
    "        return m_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd62679-7e08-4802-8112-01a1f4654578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial values for the fit, determined through a lot of testing\n",
    "# should not be changed\n",
    "initial_values = {}\n",
    "initial_values['esc_k'] = 0.015\n",
    "initial_values['ka'] = 750\n",
    "initial_values['E_2'] = 3.26315e-06\n",
    "initial_values['E_1'] = 0.532509\n",
    "initial_values['E_0'] = 0.\n",
    "initial_values['sigma_0'] = 0.53\n",
    "initial_values['c'] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbf3db-7433-4869-b730-6f3f8e4f3617",
   "metadata": {},
   "source": [
    "The fit is first performed at the K$_\\alpha$ line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02e0c1-d393-457b-9479-21c7813856b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_bins = (10_380, 10_422, 131)\n",
    "\n",
    "m = fit_model_alpha(df.cOF_ampl, initial_values, alpha_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63f6a2-010d-4852-a183-aecc86e9f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb803ee-4133-4e6e-bcd6-26244c4a9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrum_fit(df[mask_fit], m, ADC_bins=alpha_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8137df-60f6-44b8-be29-f660c1905454",
   "metadata": {},
   "source": [
    "Another fit is performed in the area around the K$_\\beta$ line, mostly using the resulting values of the K$_\\alpha$ fit and only varying the amplitude. The $\\chi^2$ value of this fit is used as an additional quality criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c136397-7a2a-4ef8-8d2e-77d67a22b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = plot_spectrum_fit(df, m, Kline='beta', E_bins=(6473, 6495, 41))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572d1e3-f3cc-44d6-b2ef-5cf61cc118f2",
   "metadata": {},
   "source": [
    "The energy resolution of the detector is defined as the full-width half-maximum (FWHM)\n",
    "$$\\Delta E_\\mathrm{FWHM} = 2 \\sqrt{2\\ln 2}\\  \\sigma_E$$\n",
    "and can be extracted from the K$_\\alpha$ fit. \n",
    "\n",
    "**Task:** Calculate the detector resolution along with its error. The results of the fit can be accessed using `Minuit.values` and `Minuit.errors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692938e3-f3ca-411e-992b-289d4aaa1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code ...\n",
    "fwhm = \n",
    "fwhm_err = \n",
    "print(f'FWHM: ({fwhm:.3f} +- {fwhm_err:.3f}) eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e37e11-4268-417f-9ce3-fa909fb5a3f3",
   "metadata": {},
   "source": [
    "**Task:** Determine the reduced $\\chi^2$ value ($\\chi^2/$ndof) for both of these fits. Discuss the obtained values. The $\\chi^2$ value can be accessed using `Minuit.fval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc12a3-5dff-4eb5-8451-30d47d9bcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code ...\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
